<!DOCTYPE html>
<html lang="zh-CN">
<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=2">
<meta name="theme-color" content="#222">
<meta name="generator" content="Hexo 4.2.0">
  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png">
  <link rel="mask-icon" href="/images/logo.svg" color="#222">

<link rel="stylesheet" href="/css/main.css">


<link rel="stylesheet" href="/lib/font-awesome/css/all.min.css">

<script id="hexo-configurations">
    var NexT = window.NexT || {};
    var CONFIG = {"hostname":"llunch4w.github.io","root":"/","scheme":"Pisces","version":"8.0.0-rc.3","exturl":false,"sidebar":{"position":"right","display":"always","padding":18,"offset":12},"copycode":true,"bookmark":{"enable":false,"color":"#222","save":"auto"},"fancybox":false,"mediumzoom":false,"lazyload":false,"pangu":false,"comments":{"style":"tabs","active":null,"storage":true,"lazyload":false,"nav":null},"algolia":{"hits":{"per_page":10},"labels":{"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}},"localsearch":{"enable":true,"trigger":"auto","top_n_per_article":1,"unescape":false,"preload":false},"motion":{"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}},"path":"search.xml"};
  </script>

  <meta name="description" content="^ _ ^">
<meta property="og:type" content="article">
<meta property="og:title" content="简单情感分类">
<meta property="og:url" content="https://llunch4w.github.io/2022/01/19/%E7%AE%80%E5%8D%95%E6%83%85%E6%84%9F%E5%88%86%E7%B1%BB/index.html">
<meta property="og:site_name" content="摸鱼的Llunch">
<meta property="og:description" content="^ _ ^">
<meta property="og:locale" content="zh_CN">
<meta property="og:image" content="https://llunch4w.github.io/2022/01/19/%E7%AE%80%E5%8D%95%E6%83%85%E6%84%9F%E5%88%86%E7%B1%BB/mlp_sc.png">
<meta property="article:published_time" content="2022-01-19T08:56:20.000Z">
<meta property="article:modified_time" content="2022-01-23T05:46:55.211Z">
<meta property="article:author" content="Llunch">
<meta property="article:tag" content="Deep Learning">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://llunch4w.github.io/2022/01/19/%E7%AE%80%E5%8D%95%E6%83%85%E6%84%9F%E5%88%86%E7%B1%BB/mlp_sc.png">

<link rel="canonical" href="https://llunch4w.github.io/2022/01/19/%E7%AE%80%E5%8D%95%E6%83%85%E6%84%9F%E5%88%86%E7%B1%BB/">


<script id="page-configurations">
  // https://hexo.io/docs/variables.html
  CONFIG.page = {
    sidebar: "",
    isHome : false,
    isPost : true,
    lang   : 'zh-CN'
  };
</script>

  <title>简单情感分类 | 摸鱼的Llunch</title>
  






  <noscript>
  <style>
  body { margin-top: 2rem; }

  .use-motion .brand,
  .use-motion .menu-item,
  .sidebar-inner,
  .use-motion .post-block,
  .use-motion .pagination,
  .use-motion .comments,
  .use-motion .post-header,
  .use-motion .post-body,
  .use-motion .collection-header { opacity: initial; }

  .use-motion .site-title,
  .use-motion .site-subtitle {
    opacity: initial;
    top: initial;
  }

  .use-motion .logo-line-before, .use-motion .logo-line-after {
    transform: scaleX(1);
  }

  .search-pop-overlay, .sidebar-nav { display: none; }
  .sidebar-panel { display: block; }
  </style>
  <div id="noscript-warning">Theme NexT works best with JavaScript enabled</div>
</noscript>

</head>

<body itemscope itemtype="http://schema.org/WebPage">
  <div class="container use-motion">
    <div class="headband"></div>

    <main class="main">
      <header class="header" itemscope itemtype="http://schema.org/WPHeader">
        <div class="header-inner"><div class="site-brand-container">
  <div class="site-nav-toggle">
    <div class="toggle" aria-label="切换导航栏">
        <span class="toggle-line toggle-line-first"></span>
        <span class="toggle-line toggle-line-middle"></span>
        <span class="toggle-line toggle-line-last"></span>
    </div>
  </div>

  <div class="site-meta">

    <a href="/" class="brand" rel="start">
      <i class="logo-line-before"></i>
      <h1 class="site-title">摸鱼的Llunch</h1>
      <i class="logo-line-after"></i>
    </a>
      <p class="site-subtitle" itemprop="description">但行好事，莫问前程</p>
  </div>

  <div class="site-nav-right">
    <div class="toggle popup-trigger">
        <i class="fa fa-search fa-fw fa-lg"></i>
    </div>
  </div>
</div>



<nav class="site-nav">
  <ul id="menu" class="main-menu menu">
        <li class="menu-item menu-item-home">

    <a href="/" rel="section"><i class="fa fa-home fa-fw"></i>首页</a>

  </li>
        <li class="menu-item menu-item-about">

    <a href="/about/" rel="section"><i class="fa fa-user fa-fw"></i>关于</a>

  </li>
        <li class="menu-item menu-item-tags">

    <a href="/tags/" rel="section"><i class="fa fa-tags fa-fw"></i>标签</a>

  </li>
        <li class="menu-item menu-item-categories">

    <a href="/categories/" rel="section"><i class="fa fa-th fa-fw"></i>分类</a>

  </li>
        <li class="menu-item menu-item-archives">

    <a href="/archives/" rel="section"><i class="fa fa-archive fa-fw"></i>归档</a>

  </li>
      <li class="menu-item menu-item-search">
        <a role="button" class="popup-trigger"><i class="fa fa-search fa-fw"></i>搜索
        </a>
      </li>
  </ul>
</nav>



  <div class="search-pop-overlay">
    <div class="popup search-popup">
        <div class="search-header">
  <span class="search-icon">
    <i class="fa fa-search"></i>
  </span>
  <div class="search-input-container">
    <input autocomplete="off" autocapitalize="off"
           placeholder="搜索..." spellcheck="false"
           type="search" class="search-input">
  </div>
  <span class="popup-btn-close">
    <i class="fa fa-times-circle"></i>
  </span>
</div>
<div id="search-result">
  <div id="no-result">
    <i class="fa fa-spinner fa-pulse fa-5x fa-fw"></i>
  </div>
</div>

    </div>
  </div>

</div>
          
  
  <div class="toggle sidebar-toggle">
    <span class="toggle-line toggle-line-first"></span>
    <span class="toggle-line toggle-line-middle"></span>
    <span class="toggle-line toggle-line-last"></span>
  </div>

  <aside class="sidebar">
    <div class="sidebar-inner">

      <ul class="sidebar-nav motion-element">
        <li class="sidebar-nav-toc">
          文章目录
        </li>
        <li class="sidebar-nav-overview">
          站点概览
        </li>
      </ul>

      <!--noindex-->
      <div class="post-toc-wrap sidebar-panel">
          <div class="post-toc motion-element"><ol class="nav"><li class="nav-item nav-level-1"><a class="nav-link" href="#Step1-Vacabulary-Mapping"><span class="nav-number">1.</span> <span class="nav-text">Step1: Vacabulary Mapping</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#Step2-Word-Embedding-Layer"><span class="nav-number">2.</span> <span class="nav-text">Step2: Word Embedding Layer</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#Step3-Word-Embedding"><span class="nav-number">3.</span> <span class="nav-text">Step3: Word Embedding</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#Step4-Data-Process"><span class="nav-number">4.</span> <span class="nav-text">Step4: Data Process</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#Step5-Create-Models"><span class="nav-number">5.</span> <span class="nav-text">Step5: Create Models</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#MLP"><span class="nav-number">5.1.</span> <span class="nav-text">MLP</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#CNN"><span class="nav-number">5.2.</span> <span class="nav-text">CNN</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#LSTM"><span class="nav-number">5.3.</span> <span class="nav-text">LSTM</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Transformer"><span class="nav-number">5.4.</span> <span class="nav-text">Transformer</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#Step6-Training"><span class="nav-number">6.</span> <span class="nav-text">Step6: Training</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#Step7-Testing"><span class="nav-number">7.</span> <span class="nav-text">Step7: Testing</span></a></li></ol></div>
      </div>
      <!--/noindex-->

      <div class="site-overview-wrap sidebar-panel">
        <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
    <img class="site-author-image" itemprop="image" alt="Llunch"
      src="/images/me.jpg">
  <p class="site-author-name" itemprop="name">Llunch</p>
  <div class="site-description" itemprop="description">Laugh through the night</div>
</div>
<div class="site-state-wrap motion-element">
  <nav class="site-state">
      <div class="site-state-item site-state-posts">
          <a href="/archives/">
        
          <span class="site-state-item-count">119</span>
          <span class="site-state-item-name">日志</span>
        </a>
      </div>
      <div class="site-state-item site-state-categories">
            <a href="/categories/">
          
        <span class="site-state-item-count">40</span>
        <span class="site-state-item-name">分类</span></a>
      </div>
      <div class="site-state-item site-state-tags">
            <a href="/tags/">
          
        <span class="site-state-item-count">79</span>
        <span class="site-state-item-name">标签</span></a>
      </div>
  </nav>
</div>
  <div class="links-of-author motion-element">
      <span class="links-of-author-item">
        <a href="https://github.com/Llunch4w" title="GitHub → https:&#x2F;&#x2F;github.com&#x2F;Llunch4w" rel="noopener" target="_blank"><i class="fab fa-github fa-fw"></i>GitHub</a>
      </span>
  </div>



      </div>
    </div>
  </aside>
  <div id="sidebar-dimmer"></div>


      </header>

      
  <div class="back-to-top">
    <i class="fa fa-arrow-up"></i>
    <span>0%</span>
  </div>


      <div class="main-inner">
        

        <div class="content post posts-expand">
          

    
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="https://llunch4w.github.io/2022/01/19/%E7%AE%80%E5%8D%95%E6%83%85%E6%84%9F%E5%88%86%E7%B1%BB/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/me.jpg">
      <meta itemprop="name" content="Llunch">
      <meta itemprop="description" content="Laugh through the night">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="摸鱼的Llunch">
    </span>
      <header class="post-header">
        <h1 class="post-title" itemprop="name headline">
          简单情感分类
        </h1>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">发表于</span>

              <time title="创建时间：2022-01-19 16:56:20" itemprop="dateCreated datePublished" datetime="2022-01-19T16:56:20+08:00">2022-01-19</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">更新于</span>
                <time title="修改时间：2022-01-23 13:46:55" itemprop="dateModified" datetime="2022-01-23T13:46:55+08:00">2022-01-23</time>
              </span>

          
            <span class="post-meta-item" title="阅读次数" id="busuanzi_container_page_pv" style="display: none;">
              <span class="post-meta-item-icon">
                <i class="fa fa-eye"></i>
              </span>
              <span class="post-meta-item-text">阅读次数：</span>
              <span id="busuanzi_value_page_pv"></span>
            </span>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
        <blockquote class="blockquote-center">
<p>^ _ ^ </p>

</blockquote>
<a id="more"></a>

<h1 id="Step1-Vacabulary-Mapping"><a href="#Step1-Vacabulary-Mapping" class="headerlink" title="Step1: Vacabulary Mapping"></a>Step1: Vacabulary Mapping</h1><p>Firstly, we need to trans natural signal(token) to integer. So we code a <strong>Vacab</strong> to record the mapping of token and index.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> collections <span class="keyword">import</span> defaultdict</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Vocab</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self)</span>:</span></span><br><span class="line">        self.idx_to_token = []</span><br><span class="line">        self.token_to_idx = &#123;&#125;</span><br><span class="line">        self.unk = <span class="string">"&lt;unk&gt;"</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">build</span><span class="params">(self, text, min_freq=<span class="number">1</span>, reserved_tokens=None)</span>:</span></span><br><span class="line">        token_freqs = defaultdict(int)</span><br><span class="line">        <span class="keyword">for</span> sentence <span class="keyword">in</span> text:</span><br><span class="line">            <span class="keyword">for</span> token <span class="keyword">in</span> sentence:</span><br><span class="line">                token_freqs[token] += <span class="number">1</span></span><br><span class="line">        uniq_tokens = [<span class="string">"&lt;unk&gt;"</span>] + (reserved_tokens <span class="keyword">if</span> reserved_tokens <span class="keyword">else</span> [])</span><br><span class="line">        uniq_tokens += [token <span class="keyword">for</span> token, freq <span class="keyword">in</span> token_freqs.items() <span class="keyword">if</span> freq &gt;= min_freq <span class="keyword">and</span> token != <span class="string">"&lt;unk&gt;"</span>]</span><br><span class="line">        <span class="comment"># TODO figure out why return cls rather object</span></span><br><span class="line">        <span class="keyword">return</span> cls(uniq_tokens)</span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__len__</span><span class="params">(self)</span>:</span></span><br><span class="line">        <span class="keyword">return</span> len(self.idx_to_token)</span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__getitem__</span><span class="params">(self, token)</span>:</span></span><br><span class="line">        <span class="keyword">return</span> self.token_to_idx.get(token, self.unk)</span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">convert_tokens_to_ids</span><span class="params">(self, tokens)</span>:</span></span><br><span class="line">        <span class="keyword">return</span> [self[token] <span class="keyword">for</span> token <span class="keyword">in</span> tokens]</span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">convert_ids_to_tokens</span><span class="params">(self, indices)</span>:</span></span><br><span class="line">        <span class="keyword">return</span> [self.idx_to_token[index] <span class="keyword">for</span> index <span class="keyword">in</span> indices]</span><br></pre></td></tr></table></figure>

<h1 id="Step2-Word-Embedding-Layer"><a href="#Step2-Word-Embedding-Layer" class="headerlink" title="Step2: Word Embedding Layer"></a>Step2: Word Embedding Layer</h1><p>Trans a word to a vector which is low dimension, dense, continues is called <strong>Embedding</strong>.  </p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># vacabulary size: 8; embedding vector dimesion: 3</span></span><br><span class="line">embedding = nn.Embedding(<span class="number">8</span>, <span class="number">3</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># input size: [2, 4]</span></span><br><span class="line">input = torch.tensor([[<span class="number">0</span>, <span class="number">1</span>, <span class="number">2</span>, <span class="number">1</span>], [<span class="number">4</span>, <span class="number">6</span>, <span class="number">6</span>, <span class="number">7</span>]], dtype=torch.long)</span><br><span class="line"><span class="comment"># output size: [2, 4, 3]</span></span><br><span class="line">output = embedding(input)</span><br></pre></td></tr></table></figure>

<h1 id="Step3-Word-Embedding"><a href="#Step3-Word-Embedding" class="headerlink" title="Step3: Word Embedding"></a>Step3: Word Embedding</h1><p>Usually, we need to trans word to word embedding vector first, then take the embedding vector as the input of MLP network. But a sequence usually contains many word vectors. The problem is <strong>How can we take them as the input vector of MLP?</strong><br>One way is <strong>Concating</strong> the n vectos as a new vector whose dimension is $n \times d$. By the way, $d$ is the dimension of word vector.<br>But this way will cause a problem: <strong>The final prediction results are too related with the position of tokens in sequence</strong>. If add a new token in the head of sequence, then all parameters of network will change and the results will be much different. </p>
<p>To solve this problem, we can use <strong>Bag Of Words</strong>. In BOW method, we don’t consider the order of words in the sequence, and simply view the sequence as a set of words. So, we can use <strong>aggregation</strong> to process many word vectors in one sequence, such as <strong>average, sum, max</strong>.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">from</span> torch <span class="keyword">import</span> nn</span><br><span class="line"><span class="keyword">from</span> torch.nn <span class="keyword">import</span> functional <span class="keyword">as</span> F</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">MLP</span><span class="params">(nn.Module)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, vocab_size, embedding_dim, hidden_dim, num_class)</span>:</span></span><br><span class="line">        super(MLP, self).__init__()</span><br><span class="line">        <span class="comment"># word vector layer</span></span><br><span class="line">        self.embedding = nn.Embedding(vocab_size, embedding_dim)</span><br><span class="line">        <span class="comment"># word vector layer --&gt; linear layer</span></span><br><span class="line">        self.linear1 = nn.Linear(embedding_dim, hidden_dim)</span><br><span class="line">        <span class="comment"># activation layer</span></span><br><span class="line">        self.activate = F.relu</span><br><span class="line">        <span class="comment"># activate layer --&gt; output layer</span></span><br><span class="line">        self.linear2 = nn.Linear(hidden_dim, num_class)</span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, inputs)</span>:</span></span><br><span class="line">        embeddings = self.embeddings(inputs)</span><br><span class="line">        <span class="comment"># aggregation</span></span><br><span class="line">        embedding = embeddings.mean(dim=<span class="number">1</span>)</span><br><span class="line">        hidden = self.activate(self.linear1(embedding))</span><br><span class="line">        outputs = self.linear2(hidden)</span><br><span class="line">        probs = F.log_softmax(outputs, dim=<span class="number">1</span>)</span><br><span class="line">        <span class="keyword">return</span> probs</span><br><span class="line"></span><br><span class="line">mlp = MLP(vocab_size=<span class="number">8</span>, embedding_dim=<span class="number">3</span>, hidden_dim=<span class="number">5</span>, num_class=<span class="number">5</span>)</span><br><span class="line"><span class="comment"># input size: [2, 4]</span></span><br><span class="line">input = torch.tensor([[<span class="number">0</span>, <span class="number">1</span>, <span class="number">2</span>, <span class="number">1</span>], [<span class="number">4</span>, <span class="number">6</span>, <span class="number">6</span>, <span class="number">7</span>]], dtype=torch.long)</span><br><span class="line"><span class="comment"># output size: [2, 2]</span></span><br><span class="line">output = mlp(input)</span><br></pre></td></tr></table></figure>

<p><img src="/2022/01/19/%E7%AE%80%E5%8D%95%E6%83%85%E6%84%9F%E5%88%86%E7%B1%BB/mlp_sc.png" alt></p>
<p>However, in real situation, input sequences in one batch often have different length. In other word, sometimes the input can not be represented by a matrix because of <strong>different sequence length</strong>.<br>A solution for this problem is <strong>EmbeddingBag</strong>:</p>
<ol>
<li>Firstly, <strong>Concating</strong> all the sequences.</li>
<li>Finally, Using <strong>Offsets</strong> to record every start position of every sequence.</li>
</ol>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Suppose every input_i has different length</span></span><br><span class="line">inputs = [input1, input2, input3, input4]</span><br><span class="line">offsets = [<span class="number">0</span>] + [i.shape[<span class="number">0</span>] <span class="keyword">for</span> i <span class="keyword">in</span> inputs]</span><br><span class="line">offsets = torch.tensor(offsets[:<span class="number">-1</span>].cumsum(dim=<span class="number">0</span>))</span><br><span class="line">inputs = torch.cat(inputs)</span><br><span class="line"></span><br><span class="line">embedding_bag = nn.EmbeddingBag(num_embeddings=<span class="number">8</span>, embedding_dim=<span class="number">3</span>)</span><br><span class="line"><span class="comment"># output size: [sequence_num, embedding_dim]</span></span><br><span class="line">embeddings = embedding_bag(inputs, offsets)</span><br></pre></td></tr></table></figure>

<p>Moreover, we can use <code>n-gram</code> as a token, which considered <strong>local information</strong> besides but added <strong>data sparsity</strong>.</p>
<h1 id="Step4-Data-Process"><a href="#Step4-Data-Process" class="headerlink" title="Step4: Data Process"></a>Step4: Data Process</h1><p><strong>create dataset</strong></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">load_sentebce_polarity</span><span class="params">()</span>:</span></span><br><span class="line">    <span class="keyword">from</span> nltk.corpus <span class="keyword">import</span> sentence_polarity</span><br><span class="line">    <span class="comment"># create vocabulary</span></span><br><span class="line">    vocab = Vocab.build(sentence_polarity.sents())</span><br><span class="line">    <span class="comment"># create train dataset</span></span><br><span class="line">    train_data = ([(vocab.convert_tokens_to_ids(sentence), <span class="number">0</span>) <span class="keyword">for</span> sentence <span class="keyword">in</span> sentence_polarity.sents(categories=<span class="string">'pos'</span>)][:<span class="number">4000</span>]</span><br><span class="line">                + [(vocab.convert_tokens_to_ids(sentence), <span class="number">1</span>) <span class="keyword">for</span> sentence <span class="keyword">in</span> sentence_polarity.sents(categories=<span class="string">'neg'</span>)][:<span class="number">4000</span>])</span><br><span class="line">    <span class="comment"># create test dataset</span></span><br><span class="line">    test_data = ([(vocab.convert_tokens_to_ids(sentence), <span class="number">0</span>) <span class="keyword">for</span> sentence <span class="keyword">in</span> sentence_polarity.sents(categories=<span class="string">'pos'</span>)][<span class="number">4000</span>:]</span><br><span class="line">                + [(vocab.convert_tokens_to_ids(sentence), <span class="number">1</span>) <span class="keyword">for</span> sentence <span class="keyword">in</span> sentence_polarity.sents(categories=<span class="string">'neg'</span>)][<span class="number">4000</span>:])</span><br><span class="line">    <span class="keyword">return</span> train_data, test_data, vocab</span><br></pre></td></tr></table></figure>

<p><strong>create data loader</strong></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> torch.utils.data <span class="keyword">import</span> DataLoader</span><br><span class="line"><span class="string">'''</span></span><br><span class="line"><span class="string">    DataLoader(dataset, batch_size, collate_fn, shuffle)</span></span><br><span class="line"><span class="string">    - dataset: a subclass of torch.utils.data.Dataset</span></span><br><span class="line"><span class="string">    - collate_fn: transform function applies in one mini-batch. e.g. transform origin data to tensor</span></span><br><span class="line"><span class="string">'''</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># dataset example</span></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">BowDataset</span><span class="params">(Dataset)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, data)</span>:</span></span><br><span class="line">        <span class="comment"># data is the original data</span></span><br><span class="line">        self.data = data</span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__len__</span><span class="params">(self)</span>:</span></span><br><span class="line">        <span class="keyword">return</span> len(self.data)</span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__getitem__</span><span class="params">(self, i)</span>:</span></span><br><span class="line">        <span class="keyword">return</span> self.data[i]</span><br><span class="line"></span><br><span class="line"><span class="comment"># collate_fn example</span></span><br><span class="line"><span class="string">'''</span></span><br><span class="line"><span class="string">    examples is a mini-batch of dastaset</span></span><br><span class="line"><span class="string">    suppose data structure is (sentence, polarity)</span></span><br><span class="line"><span class="string">    through collate_fn, we will transfer it to an input tensor</span></span><br><span class="line"><span class="string">'''</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">collate_fn</span><span class="params">(examples)</span>:</span></span><br><span class="line">    inputs = [torch.tensor(ex[<span class="number">0</span>]) <span class="keyword">for</span> ex <span class="keyword">in</span> examples]</span><br><span class="line">    targets = torch.tensor([ex[<span class="number">1</span>] <span class="keyword">for</span> ex <span class="keyword">in</span> examples], dtype=torch.long)</span><br><span class="line">    offets = [<span class="number">0</span>] + [i.shape[<span class="number">0</span>] <span class="keyword">for</span> i <span class="keyword">in</span> inputs]</span><br><span class="line">    offsets = torch.tensor(offsets[:<span class="number">-1</span>]).cumsum(dim=<span class="number">0</span>)</span><br><span class="line">    inputs = torch.cat(inputs)</span><br><span class="line">    <span class="keyword">return</span> inputs, offsets, targets</span><br></pre></td></tr></table></figure>

<h1 id="Step5-Create-Models"><a href="#Step5-Create-Models" class="headerlink" title="Step5: Create Models"></a>Step5: Create Models</h1><h2 id="MLP"><a href="#MLP" class="headerlink" title="MLP"></a>MLP</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">MLP</span><span class="params">(nn.Module)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, vocab_size, embedding_dim, hidden_dim, num_class)</span>:</span></span><br><span class="line">        super(MLP, self).__init__()</span><br><span class="line">        <span class="comment"># word vector layer</span></span><br><span class="line">        self.embedding_bag = nn.EmbeddingBag(vocab_size, embedding_dim)</span><br><span class="line">        <span class="comment"># word vector layer --&gt; linear layer</span></span><br><span class="line">        self.linear1 = nn.Linear(embedding_dim, hidden_dim)</span><br><span class="line">        <span class="comment"># activation layer</span></span><br><span class="line">        self.activate = F.relu</span><br><span class="line">        <span class="comment"># activate layer --&gt; output layer</span></span><br><span class="line">        self.linear2 = nn.Linear(hidden_dim, num_class)</span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, inputs, offsets)</span>:</span></span><br><span class="line">        embedding = self.embedding_bag(inputs, offsets)</span><br><span class="line">        hidden = self.activate(self.linear1(embedding))</span><br><span class="line">        outputs = self.linear2(hidden)</span><br><span class="line">        probs = F.log_softmax(outputs, dim=<span class="number">1</span>)</span><br><span class="line">        <span class="keyword">return</span> probs</span><br></pre></td></tr></table></figure>

<h2 id="CNN"><a href="#CNN" class="headerlink" title="CNN"></a>CNN</h2><p>In the BOW(Bag-Of-Word) Machenism, we ignored phrase meaning, such as “喜欢”, “不喜欢”. To avoid this defect, we can use CNN to extract local information.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">CNN</span><span class="params">(nn.Module)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, vocab_size, embedding_dim, hidden_dim, filter_size, num_filter, num_class)</span>:</span></span><br><span class="line">        super(CNN, self).__init__()</span><br><span class="line">        self.embedding = nn.Embedding(vocab_size, embedding_dim)</span><br><span class="line">        self.conv1d = nn.Conv1d(embedding_dim, num_filter, filter_size, padding=<span class="number">1</span>)</span><br><span class="line">        self.activate = F.relu</span><br><span class="line">        self.linear = nn.Linear(num_filter, num_class)</span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, inputs)</span>:</span></span><br><span class="line">        <span class="comment"># embedding size: [batch_size, num_seq, embedding_dim]</span></span><br><span class="line">        embedding = self.embedding(inputs)</span><br><span class="line">        <span class="comment"># after permute(0, 2, 1) size: [batch_size, embedding_dim, num_seq]</span></span><br><span class="line">        <span class="comment"># after conv1d, size: [batch_size, num_filter, num_seq+pad-kernel_size+1]</span></span><br><span class="line">        convolution = self.activate(self.conv1d(embedding.permute(<span class="number">0</span>, <span class="number">2</span>, <span class="number">1</span>)))</span><br><span class="line">        <span class="comment"># after pooling, size: [batch_size, num_filter, 1]</span></span><br><span class="line">        pooling = F.max_pool1d(convolution, kernel_size=convolution.shape[<span class="number">2</span>])</span><br><span class="line">        <span class="comment"># after squeeze, size: [batch_size, num_filter]</span></span><br><span class="line">        <span class="comment"># output size: [batch_size, num_class]</span></span><br><span class="line">        outputs = self.linear(pooling.squeeze(dim=<span class="number">2</span>))</span><br><span class="line">        <span class="comment"># log_prob size: [batch_size, num_class]</span></span><br><span class="line">        log_probs = F.log_softmax(outputs, dim=<span class="number">1</span>)</span><br><span class="line">        <span class="keyword">return</span> log_probs</span><br></pre></td></tr></table></figure>

<p>data process function need to modify some:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> torch.nn.utils.rnn <span class="keyword">import</span> pad_sequence</span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">collate_fn</span><span class="params">(examples)</span>:</span></span><br><span class="line">    inputs = [torch.tensor(ex[<span class="number">0</span>]) <span class="keyword">for</span> ex <span class="keyword">in</span> examples]</span><br><span class="line">    targets = torch.tensor([ex[<span class="number">1</span>] <span class="keyword">for</span> ex <span class="keyword">in</span> examples], dtype=torch.long)</span><br><span class="line">    <span class="comment"># padding every sequence to the same length, default char is '0'</span></span><br><span class="line">    inputs = pad_sequence(inputs, batch_first=<span class="literal">True</span>)</span><br><span class="line">    <span class="keyword">return</span> inputs, targets</span><br></pre></td></tr></table></figure>

<h2 id="LSTM"><a href="#LSTM" class="headerlink" title="LSTM"></a>LSTM</h2><p>In the BOW(Bag-Of-Word) machenism, we ignore the order of sequence. For example, pharase “张三打李四” is equal to “李四打张三” in the BOW. So BOW is not reasonable in some situations. To avoid this defect, we can use RNN, specially LSTM.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> torch.nn.utils.rnn <span class="keyword">import</span> pack_padded_sequence</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">LSTM</span><span class="params">(nn.Module)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, vocab_size, embedding_dim, hidden_dim, num_class)</span>:</span></span><br><span class="line">        super(LSTM, self).__init__()</span><br><span class="line">        self.embeddings = nn.Embedding(vocan_size, embedding_dim)</span><br><span class="line">        self.lstm = nn.LSTM(embedding_dim, hidden_dim, batch_first=<span class="literal">True</span>)</span><br><span class="line">        self.output = nn.Linear(hidden_dim, num_class)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, inputs, lengths)</span>:</span></span><br><span class="line">        embeddings = self.embeddings(inputs)</span><br><span class="line">        x_pack = pack_padded_sequence(embeddings, lengths, batch_first=<span class="literal">True</span>, enforce_sorted=<span class="literal">False</span>)</span><br><span class="line">        hidden, (hn, cn) = self.lstm(x_pack)</span><br><span class="line">        outputs = self.output(hn[<span class="number">-1</span>])</span><br><span class="line">        log_probs = F.log_softmax(outputs, dim=<span class="number">1</span>)</span><br><span class="line">        <span class="keyword">return</span> log_probs</span><br></pre></td></tr></table></figure>
<p>new data process function:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> torch.nn.utils.rnn <span class="keyword">import</span> pad_sequence</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">collate_fn</span><span class="params">(examples)</span>:</span></span><br><span class="line">    lengths = torch.tensor([len(ex[<span class="number">0</span>]) <span class="keyword">for</span> ex <span class="keyword">in</span> examples])</span><br><span class="line">    inputs = [torch.tensor(ex[<span class="number">0</span>]) <span class="keyword">for</span> ex <span class="keyword">in</span> examples]</span><br><span class="line">    targets = torch.tensor([ex[<span class="number">1</span>] <span class="keyword">for</span> ex <span class="keyword">in</span> examples], dtype=torch.long)</span><br><span class="line">    inputs = pad_sequence(inputs, batch_first=<span class="literal">True</span>)</span><br><span class="line">    <span class="keyword">return</span> inputs, lengths, target</span><br></pre></td></tr></table></figure>

<h2 id="Transformer"><a href="#Transformer" class="headerlink" title="Transformer"></a>Transformer</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Transformer</span><span class="params">(nn.Module)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, vocab_size, embedding_dim, hidden_dim, num_class, </span></span></span><br><span class="line"><span class="function"><span class="params">                    dim_forward=<span class="number">512</span>, num_head=<span class="number">2</span>, num_layers=<span class="number">2</span>, dropout=<span class="number">0.1</span>,</span></span></span><br><span class="line"><span class="function"><span class="params">                    max_len=<span class="number">128</span>, activation=<span class="string">"relu"</span>)</span>:</span></span><br><span class="line">        super(Transformer, self).__init__()</span><br><span class="line">        self.embedding_dim = embedding_dim</span><br><span class="line">        self.embeddings = nn.Embedding(vocab_size, embedding_dim)</span><br><span class="line">        self.position_embedding = PositionalEncdoing(embedding_dim, dropout, max_len)</span><br><span class="line">        encoder_layer = nn.TransformerEncoderLayer(hidden_dim, num_head, dim_feedforward, dropout, activation)</span><br><span class="line">        self.transformer = nn.TransformerEncoder(encoder_layer, num_layers)</span><br><span class="line">        self.output = nn.Linear(hidden_dim, num_class)</span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, inputs, lengths)</span>:</span></span><br><span class="line">        <span class="comment"># [batch_size, num_seq, ...] to [num_seq, batch_size, ...]</span></span><br><span class="line">        inputs = torch.transpose(inputs, <span class="number">0</span>, <span class="number">1</span>)</span><br><span class="line">        hidden_states = self.embeddings(inputs)</span><br><span class="line">        hidden_states = self.position_embedding(hidden_states)</span><br><span class="line">        <span class="comment"># no-attention part would be true</span></span><br><span class="line">        attention_mask = length_to_mask(lengths) == <span class="literal">False</span></span><br><span class="line">        hidden_states = self.transformer(hidden_states, src_key_padding_mask=attention_mask)</span><br><span class="line">        hidden_states = hidden_states[<span class="number">0</span>, :, :]</span><br><span class="line">        output = self.output(hidden_states)</span><br><span class="line">        log_probs = F.log_softmax(output, dim=<span class="number">1</span>)</span><br><span class="line">        <span class="keyword">return</span> log_probs</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">length_to_mask</span><span class="params">(lengths)</span>:</span></span><br><span class="line">    <span class="string">'''transfer sequence length to mask matrix</span></span><br><span class="line"><span class="string">        @param lengths: [batch, ]</span></span><br><span class="line"><span class="string">        @return [batch, max_len]</span></span><br><span class="line"><span class="string">    '''</span></span><br><span class="line">    max_len = torch.max(lengths)</span><br><span class="line">    mask = torch.arange(max_len).expand(lengths.shape[<span class="number">0</span>], max_len) &lt; lengths.unsqueeze(<span class="number">1</span>)</span><br><span class="line">    <span class="keyword">return</span> mask</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">PositionalEncoding</span><span class="params">(nn.Module)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, d_model, dropout=<span class="number">0.1</span>, max_len=<span class="number">512</span>)</span>:</span></span><br><span class="line">        super(PositionalEncoding, self).__init__()</span><br><span class="line">        pe = torch.zeros(max_len, d_model)</span><br><span class="line">        position = torch.arange(<span class="number">0</span>, max_len, dtype=torch.float).unsqueeze(<span class="number">1</span>)</span><br><span class="line">        div_term = torch.exo(torch.arange(<span class="number">0</span>, d_model, <span class="number">2</span>).float() * (-math.log(<span class="number">10000.0</span>)/d_model))</span><br><span class="line">        <span class="comment"># position encoding for even position</span></span><br><span class="line">        pe[:, <span class="number">0</span>::<span class="number">2</span>] = torch.sin(position * div_term)</span><br><span class="line">        <span class="comment"># position encoding for odd position</span></span><br><span class="line">        pe[:, <span class="number">1</span>:<span class="number">2</span>] = torch.cos(position * div_term)</span><br><span class="line">        pe = pe.unsqueeze(<span class="number">0</span>).transpose(<span class="number">0</span>, <span class="number">1</span>)</span><br><span class="line">        <span class="comment"># no gradient operation in positiong embedding</span></span><br><span class="line">        self.register_buffer(<span class="string">'pe'</span>, pe)</span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, x)</span>:</span></span><br><span class="line">        x = x + self.pe[:x.size(<span class="number">0</span>), :]</span><br><span class="line">        <span class="keyword">return</span> x</span><br></pre></td></tr></table></figure>

<h1 id="Step6-Training"><a href="#Step6-Training" class="headerlink" title="Step6: Training"></a>Step6: Training</h1><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> tqdm.auto <span class="keyword">import</span> tqdm</span><br><span class="line"></span><br><span class="line"><span class="comment"># hyper-parameter</span></span><br><span class="line">embedding_dim = <span class="number">128</span></span><br><span class="line">hidden_dim = <span class="number">256</span></span><br><span class="line">num_class = <span class="number">2</span></span><br><span class="line">batch_size = <span class="number">32</span></span><br><span class="line">num_epoch = <span class="number">5</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># load data</span></span><br><span class="line">train_data, test_data, vocab = load_sentence_polarity()</span><br><span class="line">trian_dataset = BowDataset(train_data)</span><br><span class="line">test_dataset = BowDataset(test_data)</span><br><span class="line">train_data_loader = DataLoader(train_dataset, batch_size=batch_size, collate_fn=collate_fn, shuffle=<span class="literal">True</span>)</span><br><span class="line">test_data_loader = DataLoader(test_dataset, batch_size=<span class="number">1</span>, collate_fn=collate_fn, shuffle=<span class="literal">False</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># create model</span></span><br><span class="line">device = torch.device(<span class="string">'cuda'</span> <span class="keyword">if</span> torch.cuda.is_available() <span class="keyword">else</span> <span class="string">'cpu'</span>)</span><br><span class="line">model = MLP(len(vocab), embedding_dim, hidden_dim, num_class)</span><br><span class="line">model.to(device)</span><br><span class="line"></span><br><span class="line"><span class="comment"># traning</span></span><br><span class="line">nll_loss = nn.NLLLoss()</span><br><span class="line">optimizer = optim.Adm(model.parameters(), lr=<span class="number">0.001</span>)</span><br><span class="line">model.train()</span><br><span class="line"><span class="keyword">for</span> epoch <span class="keyword">in</span> range(num_epoch):</span><br><span class="line">    total_loss = <span class="number">0</span></span><br><span class="line">    <span class="keyword">for</span> batch <span class="keyword">in</span> tqdm(trian_data_loader, desc=<span class="string">f"Training Epoch <span class="subst">&#123;epoch&#125;</span>"</span>):</span><br><span class="line">        inputs, offsets, targets = [x.to(device) <span class="keyword">for</span> x <span class="keyword">in</span> batch]</span><br><span class="line">        log_probs = model(inputs, offsets)</span><br><span class="line">        loss = nll_loss(log_probs, targets)</span><br><span class="line">        optimizer.zero_grad()</span><br><span class="line">        loss.backward()</span><br><span class="line">        optimizer.step()</span><br><span class="line">        total_loss += loss.item()</span><br><span class="line">    print(<span class="string">f"Loss: <span class="subst">&#123;total_loss:<span class="number">.2</span>f&#125;</span>"</span>)</span><br></pre></td></tr></table></figure>

<h1 id="Step7-Testing"><a href="#Step7-Testing" class="headerlink" title="Step7: Testing"></a>Step7: Testing</h1><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">acc = <span class="number">0</span></span><br><span class="line"><span class="keyword">for</span> batch <span class="keyword">in</span> tqdm(test_data_loader, desc=<span class="string">"Testing"</span>):</span><br><span class="line">    inputs, offsets, targets = [x.to(device) <span class="keyword">for</span> x <span class="keyword">in</span> batch]</span><br><span class="line">    <span class="keyword">with</span> torch.no_grad():</span><br><span class="line">        output = model(inputs, offsets)</span><br><span class="line">        acc += (output.argmax(dim=<span class="number">1</span>) == target).sum().item()</span><br><span class="line">print(<span class="string">f"Acc: <span class="subst">&#123;acc / len(test_data_loader):<span class="number">.2</span>f&#125;</span>"</span>)</span><br></pre></td></tr></table></figure>
    </div>

    
    
    

      <footer class="post-footer">
          <div class="post-tags">
              <a href="/tags/Deep-Learning/" rel="tag"># Deep Learning</a>
          </div>

        


        
    <div class="post-nav">
      <div class="post-nav-item">
    <a href="/2022/01/18/Python-API/" rel="prev" title="Python API">
      <i class="fa fa-chevron-left"></i> Python API
    </a></div>
      <div class="post-nav-item">
    <a href="/2022/01/20/%E4%B8%BAhexo-next%E6%B7%BB%E5%8A%A0%E8%83%8C%E6%99%AF/" rel="next" title="为hexo-next添加背景">
      为hexo-next添加背景 <i class="fa fa-chevron-right"></i>
    </a></div>
    </div>
      </footer>
    
  </article>
  
  
  



        </div>
        
    
  <div class="comments">
    <div id="lv-container" data-id="city" data-uid="MTAyMC81MDY4OC8yNzE3MQ"></div>
  </div>
  

<script>
  window.addEventListener('tabs:register', () => {
    let { activeClass } = CONFIG.comments;
    if (CONFIG.comments.storage) {
      activeClass = localStorage.getItem('comments_active') || activeClass;
    }
    if (activeClass) {
      let activeTab = document.querySelector(`a[href="#comment-${activeClass}"]`);
      if (activeTab) {
        activeTab.click();
      }
    }
  });
  if (CONFIG.comments.storage) {
    window.addEventListener('tabs:click', event => {
      if (!event.target.matches('.tabs-comment .tab-content .tab-pane')) return;
      let commentClass = event.target.classList[1];
      localStorage.setItem('comments_active', commentClass);
    });
  }
</script>

      </div>
    </main>

    <footer class="footer">
      <div class="footer-inner">
        

        

<div class="copyright">
  
  &copy; 
  <span itemprop="copyrightYear">2022</span>
  <span class="with-love">
    <i class="fa fa-heart"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">Llunch</span>
</div>
  <div class="powered-by">由 <a href="https://hexo.io/" class="theme-link" rel="noopener" target="_blank">Hexo</a> & <a href="https://theme-next.js.org/pisces/" class="theme-link" rel="noopener" target="_blank">NexT.Pisces</a> 强力驱动
  </div>

        
<div class="busuanzi-count">
  <script async src="https://busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script>
    <span class="post-meta-item" id="busuanzi_container_site_uv" style="display: none;">
      <span class="post-meta-item-icon">
        <i class="fa fa-user"></i>
      </span>
      <span class="site-uv" title="总访客量">
        <span id="busuanzi_value_site_uv"></span>
      </span>
    </span>
    <span class="post-meta-divider">|</span>
    <span class="post-meta-item" id="busuanzi_container_site_pv" style="display: none;">
      <span class="post-meta-item-icon">
        <i class="fa fa-eye"></i>
      </span>
      <span class="site-pv" title="总访问量">
        <span id="busuanzi_value_site_pv"></span>
      </span>
    </span>
</div>








      </div>
    </footer>
  </div>

  
  <script src="/lib/anime.min.js"></script>
  <script src="/lib/velocity/velocity.min.js"></script>
  <script src="/lib/velocity/velocity.ui.min.js"></script>

<script src="/js/utils.js"></script>

<script src="/js/motion.js"></script>


<script src="/js/next-boot.js"></script>


  




  
<script src="/js/local-search.js"></script>













  

  
      

<script>
  if (typeof MathJax === 'undefined') {
    window.MathJax = {
      loader: {
        source: {
          '[tex]/amsCd': '[tex]/amscd',
          '[tex]/AMScd': '[tex]/amscd'
        }
      },
      tex: {
        inlineMath: {'[+]': [['$', '$']]},
        tags: 'ams'
      },
      options: {
        renderActions: {
          findScript: [10, doc => {
            document.querySelectorAll('script[type^="math/tex"]').forEach(node => {
              const display = !!node.type.match(/; *mode=display/);
              const math = new doc.options.MathItem(node.textContent, doc.inputJax[0], display);
              const text = document.createTextNode('');
              node.parentNode.replaceChild(text, node);
              math.start = {node: text, delim: '', n: 0};
              math.end = {node: text, delim: '', n: 0};
              doc.math.push(math);
            });
          }, '', false],
          insertedScript: [200, () => {
            document.querySelectorAll('mjx-container').forEach(node => {
              let target = node.parentNode;
              if (target.nodeName.toLowerCase() === 'li') {
                target.parentNode.classList.add('has-jax');
              }
            });
          }, '', false]
        }
      }
    };
    (function () {
      var script = document.createElement('script');
      script.src = '//cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js';
      script.defer = true;
      document.head.appendChild(script);
    })();
  } else {
    MathJax.startup.document.state(0);
    MathJax.texReset();
    MathJax.typeset();
  }
</script>

    

  

<script>
NexT.utils.loadComments(document.querySelector('#lv-container'), () => {
  window.livereOptions = {
    refer: location.pathname.replace(CONFIG.root, '').replace('index.html', '')
  };
  (function(d, s) {
    var j, e = d.getElementsByTagName(s)[0];
    if (typeof LivereTower === 'function') { return; }
    j = d.createElement(s);
    j.src = 'https://cdn-city.livere.com/js/embed.dist.js';
    j.async = true;
    e.parentNode.insertBefore(j, e);
  })(document, 'script');
});
</script>

<script src="/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"pluginModelPath":"assets/","model":{"jsonPath":"/live2dw/assets/koharu.model.json"},"display":{"position":"right","width":200,"height":400},"mobile":{"show":false},"rect":"opacity:0.7","log":false,"pluginJsPath":"lib/","pluginRootPath":"live2dw/","tagMode":false});</script></body>
</html>
