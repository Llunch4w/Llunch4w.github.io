<!DOCTYPE html>
<html lang="zh-CN">
<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=2">
<meta name="theme-color" content="#222">
<meta name="generator" content="Hexo 4.2.0">
  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png">
  <link rel="mask-icon" href="/images/logo.svg" color="#222">

<link rel="stylesheet" href="/css/main.css">


<link rel="stylesheet" href="/lib/font-awesome/css/all.min.css">

<script id="hexo-configurations">
    var NexT = window.NexT || {};
    var CONFIG = {"hostname":"llunch4w.gitee.io","root":"/","scheme":"Pisces","version":"8.0.0-rc.3","exturl":false,"sidebar":{"position":"right","display":"always","padding":18,"offset":12},"copycode":true,"bookmark":{"enable":false,"color":"#222","save":"auto"},"fancybox":false,"mediumzoom":false,"lazyload":false,"pangu":false,"comments":{"style":"tabs","active":null,"storage":true,"lazyload":false,"nav":null},"algolia":{"hits":{"per_page":10},"labels":{"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}},"localsearch":{"enable":true,"trigger":"auto","top_n_per_article":1,"unescape":false,"preload":false},"motion":{"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}},"path":"search.xml"};
  </script>

  <meta name="description" content="^ _ ^">
<meta property="og:type" content="article">
<meta property="og:title" content="注意力模型">
<meta property="og:url" content="https://llunch4w.gitee.io/2022/01/17/%E6%B3%A8%E6%84%8F%E5%8A%9B%E6%A8%A1%E5%9E%8B/index.html">
<meta property="og:site_name" content="摸鱼的Llunch">
<meta property="og:description" content="^ _ ^">
<meta property="og:locale" content="zh_CN">
<meta property="og:image" content="https://llunch4w.gitee.io/2022/01/17/%E6%B3%A8%E6%84%8F%E5%8A%9B%E6%A8%A1%E5%9E%8B/seq2seq.png">
<meta property="og:image" content="https://llunch4w.gitee.io/2022/01/17/%E6%B3%A8%E6%84%8F%E5%8A%9B%E6%A8%A1%E5%9E%8B/attention_seq2seq.png">
<meta property="og:image" content="https://llunch4w.gitee.io/2022/01/17/%E6%B3%A8%E6%84%8F%E5%8A%9B%E6%A8%A1%E5%9E%8B/attention_cal.png">
<meta property="og:image" content="https://llunch4w.gitee.io/2022/01/17/%E6%B3%A8%E6%84%8F%E5%8A%9B%E6%A8%A1%E5%9E%8B/position_encoding.png">
<meta property="og:image" content="https://llunch4w.gitee.io/2022/01/17/%E6%B3%A8%E6%84%8F%E5%8A%9B%E6%A8%A1%E5%9E%8B/transformer_block.png">
<meta property="og:image" content="https://llunch4w.gitee.io/2022/01/17/%E6%B3%A8%E6%84%8F%E5%8A%9B%E6%A8%A1%E5%9E%8B/seq2seq_transformer.png">
<meta property="article:published_time" content="2022-01-17T07:56:45.000Z">
<meta property="article:modified_time" content="2022-01-19T09:49:38.984Z">
<meta property="article:author" content="Llunch">
<meta property="article:tag" content="Deep Learning">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://llunch4w.gitee.io/2022/01/17/%E6%B3%A8%E6%84%8F%E5%8A%9B%E6%A8%A1%E5%9E%8B/seq2seq.png">

<link rel="canonical" href="https://llunch4w.gitee.io/2022/01/17/%E6%B3%A8%E6%84%8F%E5%8A%9B%E6%A8%A1%E5%9E%8B/">


<script id="page-configurations">
  // https://hexo.io/docs/variables.html
  CONFIG.page = {
    sidebar: "",
    isHome : false,
    isPost : true,
    lang   : 'zh-CN'
  };
</script>

  <title>注意力模型 | 摸鱼的Llunch</title>
  






  <noscript>
  <style>
  body { margin-top: 2rem; }

  .use-motion .brand,
  .use-motion .menu-item,
  .sidebar-inner,
  .use-motion .post-block,
  .use-motion .pagination,
  .use-motion .comments,
  .use-motion .post-header,
  .use-motion .post-body,
  .use-motion .collection-header { opacity: initial; }

  .use-motion .site-title,
  .use-motion .site-subtitle {
    opacity: initial;
    top: initial;
  }

  .use-motion .logo-line-before, .use-motion .logo-line-after {
    transform: scaleX(1);
  }

  .search-pop-overlay, .sidebar-nav { display: none; }
  .sidebar-panel { display: block; }
  </style>
  <div id="noscript-warning">Theme NexT works best with JavaScript enabled</div>
</noscript>

</head>

<body itemscope itemtype="http://schema.org/WebPage">
  <div class="container use-motion">
    <div class="headband"></div>

    <main class="main">
      <header class="header" itemscope itemtype="http://schema.org/WPHeader">
        <div class="header-inner"><div class="site-brand-container">
  <div class="site-nav-toggle">
    <div class="toggle" aria-label="切换导航栏">
        <span class="toggle-line toggle-line-first"></span>
        <span class="toggle-line toggle-line-middle"></span>
        <span class="toggle-line toggle-line-last"></span>
    </div>
  </div>

  <div class="site-meta">

    <a href="/" class="brand" rel="start">
      <i class="logo-line-before"></i>
      <h1 class="site-title">摸鱼的Llunch</h1>
      <i class="logo-line-after"></i>
    </a>
  </div>

  <div class="site-nav-right">
    <div class="toggle popup-trigger">
        <i class="fa fa-search fa-fw fa-lg"></i>
    </div>
  </div>
</div>



<nav class="site-nav">
  <ul id="menu" class="main-menu menu">
        <li class="menu-item menu-item-home">

    <a href="/" rel="section"><i class="fa fa-home fa-fw"></i>首页</a>

  </li>
        <li class="menu-item menu-item-about">

    <a href="/about/" rel="section"><i class="fa fa-user fa-fw"></i>关于</a>

  </li>
        <li class="menu-item menu-item-tags">

    <a href="/tags/" rel="section"><i class="fa fa-tags fa-fw"></i>标签</a>

  </li>
        <li class="menu-item menu-item-categories">

    <a href="/categories/" rel="section"><i class="fa fa-th fa-fw"></i>分类</a>

  </li>
        <li class="menu-item menu-item-archives">

    <a href="/archives/" rel="section"><i class="fa fa-archive fa-fw"></i>归档</a>

  </li>
      <li class="menu-item menu-item-search">
        <a role="button" class="popup-trigger"><i class="fa fa-search fa-fw"></i>搜索
        </a>
      </li>
  </ul>
</nav>



  <div class="search-pop-overlay">
    <div class="popup search-popup">
        <div class="search-header">
  <span class="search-icon">
    <i class="fa fa-search"></i>
  </span>
  <div class="search-input-container">
    <input autocomplete="off" autocapitalize="off"
           placeholder="搜索..." spellcheck="false"
           type="search" class="search-input">
  </div>
  <span class="popup-btn-close">
    <i class="fa fa-times-circle"></i>
  </span>
</div>
<div id="search-result">
  <div id="no-result">
    <i class="fa fa-spinner fa-pulse fa-5x fa-fw"></i>
  </div>
</div>

    </div>
  </div>

</div>
          
  
  <div class="toggle sidebar-toggle">
    <span class="toggle-line toggle-line-first"></span>
    <span class="toggle-line toggle-line-middle"></span>
    <span class="toggle-line toggle-line-last"></span>
  </div>

  <aside class="sidebar">
    <div class="sidebar-inner">

      <ul class="sidebar-nav motion-element">
        <li class="sidebar-nav-toc">
          文章目录
        </li>
        <li class="sidebar-nav-overview">
          站点概览
        </li>
      </ul>

      <!--noindex-->
      <div class="post-toc-wrap sidebar-panel">
          <div class="post-toc motion-element"><ol class="nav"><li class="nav-item nav-level-1"><a class="nav-link" href="#Motivation"><span class="nav-number">1.</span> <span class="nav-text">Motivation</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#Seq2Seq"><span class="nav-number">1.1.</span> <span class="nav-text">Seq2Seq</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#RNN-for-Seq2Seq"><span class="nav-number">1.2.</span> <span class="nav-text">RNN for Seq2Seq</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#Attention-Mechanism"><span class="nav-number">2.</span> <span class="nav-text">Attention Mechanism</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#Self-Attention"><span class="nav-number">3.</span> <span class="nav-text">Self Attention</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#Transformer"><span class="nav-number">4.</span> <span class="nav-text">Transformer</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#Motivation-1"><span class="nav-number">4.1.</span> <span class="nav-text">Motivation</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Transformer-Mechanism"><span class="nav-number">4.2.</span> <span class="nav-text">Transformer Mechanism</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#Importing-Position-Information"><span class="nav-number">4.2.1.</span> <span class="nav-text">Importing Position Information</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Importing-Role-Information"><span class="nav-number">4.2.2.</span> <span class="nav-text">Importing Role Information</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Multiple-Layer-Attention"><span class="nav-number">4.2.3.</span> <span class="nav-text">Multiple Layer Attention</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Multi-Head-Machenism"><span class="nav-number">4.2.4.</span> <span class="nav-text">Multi-Head Machenism</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Seq2Seq-based-on-Transformer"><span class="nav-number">4.3.</span> <span class="nav-text">Seq2Seq based on Transformer</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Pros-amp-Cons"><span class="nav-number">4.4.</span> <span class="nav-text">Pros &amp; Cons</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#Code"><span class="nav-number">5.</span> <span class="nav-text">Code</span></a></li></ol></div>
      </div>
      <!--/noindex-->

      <div class="site-overview-wrap sidebar-panel">
        <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
    <img class="site-author-image" itemprop="image" alt="Llunch"
      src="/images/me.jpg">
  <p class="site-author-name" itemprop="name">Llunch</p>
  <div class="site-description" itemprop="description">没有理想的人不伤心</div>
</div>
<div class="site-state-wrap motion-element">
  <nav class="site-state">
      <div class="site-state-item site-state-posts">
          <a href="/archives/">
        
          <span class="site-state-item-count">100</span>
          <span class="site-state-item-name">日志</span>
        </a>
      </div>
      <div class="site-state-item site-state-categories">
            <a href="/categories/">
          
        <span class="site-state-item-count">36</span>
        <span class="site-state-item-name">分类</span></a>
      </div>
      <div class="site-state-item site-state-tags">
            <a href="/tags/">
          
        <span class="site-state-item-count">73</span>
        <span class="site-state-item-name">标签</span></a>
      </div>
  </nav>
</div>
  <div class="links-of-author motion-element">
      <span class="links-of-author-item">
        <a href="https://github.com/Llunch4w" title="GitHub → https:&#x2F;&#x2F;github.com&#x2F;Llunch4w" rel="noopener" target="_blank"><i class="fab fa-github fa-fw"></i>GitHub</a>
      </span>
  </div>



      </div>
    </div>
  </aside>
  <div id="sidebar-dimmer"></div>


      </header>

      
  <div class="back-to-top">
    <i class="fa fa-arrow-up"></i>
    <span>0%</span>
  </div>


      <div class="main-inner">
        

        <div class="content post posts-expand">
          

    
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="https://llunch4w.gitee.io/2022/01/17/%E6%B3%A8%E6%84%8F%E5%8A%9B%E6%A8%A1%E5%9E%8B/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/me.jpg">
      <meta itemprop="name" content="Llunch">
      <meta itemprop="description" content="没有理想的人不伤心">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="摸鱼的Llunch">
    </span>
      <header class="post-header">
        <h1 class="post-title" itemprop="name headline">
          注意力模型
        </h1>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">发表于</span>

              <time title="创建时间：2022-01-17 15:56:45" itemprop="dateCreated datePublished" datetime="2022-01-17T15:56:45+08:00">2022-01-17</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">更新于</span>
                <time title="修改时间：2022-01-19 17:49:38" itemprop="dateModified" datetime="2022-01-19T17:49:38+08:00">2022-01-19</time>
              </span>

          
            <span class="post-meta-item" title="阅读次数" id="busuanzi_container_page_pv" style="display: none;">
              <span class="post-meta-item-icon">
                <i class="fa fa-eye"></i>
              </span>
              <span class="post-meta-item-text">阅读次数：</span>
              <span id="busuanzi_value_page_pv"></span>
            </span>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
        <blockquote class="blockquote-center">
<p>^ _ ^ </p>

</blockquote>
<a id="more"></a>
<h1 id="Motivation"><a href="#Motivation" class="headerlink" title="Motivation"></a>Motivation</h1><h2 id="Seq2Seq"><a href="#Seq2Seq" class="headerlink" title="Seq2Seq"></a>Seq2Seq</h2><p><img src="/2022/01/17/%E6%B3%A8%E6%84%8F%E5%8A%9B%E6%A8%A1%E5%9E%8B/seq2seq.png" alt></p>
<p>Above figure show an application which uses <strong>Seq2Seq Model</strong> to realize <strong>Machine Translation</strong>. Seq2Seq model is composed of 2 parts, respectivly <strong>Encoder</strong> and <strong>Decoder</strong> : </p>
<ol>
<li>origin sequence encodes to one coding.</li>
<li>Then decode the coding to the target sequence.</li>
</ol>
<h2 id="RNN-for-Seq2Seq"><a href="#RNN-for-Seq2Seq" class="headerlink" title="RNN for Seq2Seq"></a>RNN for Seq2Seq</h2><p>Besides <strong>Classfication</strong> and <strong>Sequence Labeling</strong>, RNN can also solved <strong>Reading Comprehension</strong> and <strong>Text Generation</strong>, which is corresponded to <strong>Seq2Seq Model</strong>.<br>Seq2Seq based on RNN:</p>
<ol>
<li>Use RNN to encode origin sequence.</li>
<li>Use the last hidden state of encoder RNN as the input, invoking decoder RNN to generate target sequence word by word.    </li>
</ol>
<p>There is a basic hypothesis for Seq2Seq based on RNN: <strong>The last hidden state in the encoder RNN contains all the information of the origin sequence</strong>.<br>Obviously, the hypothesis is unreasonable, especially for long origin sequence.     </p>
<p>To solve the problem, someone proposed <strong>Attention Model</strong>.</p>
<h1 id="Attention-Mechanism"><a href="#Attention-Mechanism" class="headerlink" title="Attention Mechanism"></a>Attention Mechanism</h1><p>The core idea of <strong>Attention Machinism</strong>: When generating a new target word, not noly considering previous time state and already generated word, but also considering which words in the origin sequence is more relevant to the current word need to generate. It guides us put more attention to the words in the source sequence which is more relevant to the current word.</p>
<p><img src="/2022/01/17/%E6%B3%A8%E6%84%8F%E5%8A%9B%E6%A8%A1%E5%9E%8B/attention_seq2seq.png" alt></p>
<p>Mathmatically,<br>
$$
\begin{matrix}
    \hat{\alpha_s} = attn(h_s, h_{t-1}) \\
    \alpha_s = Softmax(\hat{\alpha})_s
\end{matrix}
$$
    </p>
<ul>
<li>$h_s$: the state of time $s$ in the source sequence.</li>
<li>$h_{t-1}$: the state of previous time in the target sequence.</li>
<li>$attn$: attention calculation formula.</li>
<li>$\hat{\alpha} = [\hat{\alpha}_1, \hat{\alpha}_2, \cdots, \hat{\alpha}_L]$, where $L$ is the length of source sequence.</li>
<li>$Softmax$: Normalize attention score.</li>
</ul>
<p><strong>Attention Calculation Formula</strong><br><img src="/2022/01/17/%E6%B3%A8%E6%84%8F%E5%8A%9B%E6%A8%A1%E5%9E%8B/attention_cal.png" alt></p>
<h1 id="Self-Attention"><a href="#Self-Attention" class="headerlink" title="Self Attention"></a>Self Attention</h1><p>The core idea of <strong>Self-Attention</strong>: Observe its companion, then know its meaning. In other word, the state of one point in the sequence can be calculated by the correlation(attention) between the state at this time and the state at other times.</p>
<p>Specifically,   </p>
<ul>
<li>inputs can be represented by one combination of n vectors: $x_1, x_2, \cdots, x_n$.</li>
<li>outputs can be represented by one combination of n new vectors: $y_1, y_2, \cdots, y_n$.</li>
<li>The calculation formulate of $y<em>i$: $y_i = \sum</em>{j=1}^n a<em>{ij}x</em>{j}$<ul>
<li>$a_{ij}$ is the attention weight(after softmax normalize) between $x_i$ and $x_j$.</li>
</ul>
</li>
</ul>
<p>Through the self-attention machanism, the relationship between two distant moments can be directly calculated.</p>
<h1 id="Transformer"><a href="#Transformer" class="headerlink" title="Transformer"></a>Transformer</h1><h2 id="Motivation-1"><a href="#Motivation-1" class="headerlink" title="Motivation"></a>Motivation</h2><p>There are some problems needed to fix of <strong>Self-Attention Model</strong>.</p>
<ul>
<li>There is no consideration about <strong>position information</strong> when calculating <strong>self attention</strong>.</li>
<li>input vector $x_i$ is responsible for 3 roles, which caused it hard to learn: <ul>
<li>one of two vectors when calculating attention weight.</li>
<li>the weighted vector(被加权的向量)</li>
</ul>
</li>
<li>only considering relationship of 2 units, but not be able to represent multiple relationship between multiple input units.</li>
<li>the results of self-attention are mutually exclusive, and multiple inputs can not be concerned at the same time.</li>
</ul>
<p>Transformer can solve these problems.</p>
<h2 id="Transformer-Mechanism"><a href="#Transformer-Mechanism" class="headerlink" title="Transformer Mechanism"></a>Transformer Mechanism</h2><p>Just as its name implies, Transformer transfer a vector sequence into another vector sequence.</p>
<h3 id="Importing-Position-Information"><a href="#Importing-Position-Information" class="headerlink" title="Importing Position Information"></a>Importing Position Information</h3><p>We need to import position information for every input vector. There are 2 ways to import position information:</p>
<ol>
<li>Position Embeddings(位置嵌入): Like word embedding, use a continues, lower dimension, dense vector to represent position information.</li>
<li>Position Encodings(位置编码): Use function to map a position index(integer) to a vector.<br><img src="/2022/01/17/%E6%B3%A8%E6%84%8F%E5%8A%9B%E6%A8%A1%E5%9E%8B/position_encoding.png" alt></li>
</ol>
<ul>
<li>$p$ indicates the position index.</li>
<li>$0 \leq i \lt d$ is the index of encoding vector. The dimension of the vector is $d$.</li>
</ul>
<p>Whatever use <strong>Position Embeddings</strong> or <strong>Position Encodings</strong>, after we get a position vector, we add it into the word vecotr, then the result is the final input vector. In result of importing position information, same word in different position will be represented by different vector.</p>
<h3 id="Importing-Role-Information"><a href="#Importing-Role-Information" class="headerlink" title="Importing Role Information"></a>Importing Role Information</h3><p>In the original Attention Machenism, a input vector acts 3 roles: <strong>Query</strong>, <strong>Key</strong>, <strong>Value</strong>. The better way is represent different role with different vector. </p>
<p>The core idea of import role information is <strong>Different Linear Transforamtion for Input Vector</strong>. By the way, a linear transformation can be represented by a <strong>Parameter Matrix</strong>.</p>
<p>Input vector $x_i$ will map to 3 new vectors by 3 different parameter matrixes:</p>
<ul>
<li>$q_i = W^q x_i$</li>
<li>$k_i = W^k x_i$</li>
<li>$v_i = W^v x_i$</li>
</ul>
<p><strong>New Calculation Formulation</strong><br>
$$
\begin{matrix}
y_i = \sum_{j=1}^n = \alpha_{ij}v_j \\
\alpha_{ij} = Softmax(\hat{\alpha}_i)_j, \hat{\alpha}_{i} = [\hat{\alpha}_{i1},\hat{\alpha}_{i2},\cdots,\hat{\alpha}_{iL}] \\
\hat{\alpha}_{ij} = attn(q_i, k_j)
\end{matrix}
$$
    </p>
<h3 id="Multiple-Layer-Attention"><a href="#Multiple-Layer-Attention" class="headerlink" title="Multiple Layer Attention"></a>Multiple Layer Attention</h3><p>In the original Attention Machenism, it only considers the relationship between 2 input sequence units. But in practical applications, it is often necessary to consider the relationship between many sequence units. If we consturct higher order relationship, it will result higher model complexity. </p>
<p>To solve this demands, someone proposed <strong>Message Propagation</strong>, which can realized by stacking multiple layer self-attention model. Normally, attention calculation use linear function. So the result of stacking them will also be linear. In order to enhance the presentation ability of model, we can add non-linear MLP behind every attention layer.</p>
<p>In addition, in order to benefit the process of model learning, we can also use some techniques, such as <strong>Layer Normalization</strong>, <strong>Residual Connections</strong>.</p>
<p>All the layers stacked to construct a <strong>Transformer Block</strong>.<br><img src="/2022/01/17/%E6%B3%A8%E6%84%8F%E5%8A%9B%E6%A8%A1%E5%9E%8B/transformer_block.png" alt></p>
<h3 id="Multi-Head-Machenism"><a href="#Multi-Head-Machenism" class="headerlink" title="Multi-Head Machenism"></a>Multi-Head Machenism</h3><p>Since the result of self-attention need to be normalized, even if an input is related to multiple other inputs, they can not be given a large attention weight at the same time. In the other word, the results of self-attention is <strong>exclusive</strong>.</p>
<p>To solve this problem, the core idea is:</p>
<ul>
<li>Set multiple groups of mapping matrixes.</li>
<li>Concating all the outputs of every results after those mapping.</li>
<li>Through a linear mappping, map the concated output to a vector of dimension d.</li>
</ul>
<p>In other view, <strong>Multi-Head Attention Machenism</strong> is equal to <strong>Ensemble of multiple Self-Attention Model</strong>, like different kernels in the CNN. </p>
<h2 id="Seq2Seq-based-on-Transformer"><a href="#Seq2Seq-based-on-Transformer" class="headerlink" title="Seq2Seq based on Transformer"></a>Seq2Seq based on Transformer</h2><p><img src="/2022/01/17/%E6%B3%A8%E6%84%8F%E5%8A%9B%E6%A8%A1%E5%9E%8B/seq2seq_transformer.png" alt></p>
<h2 id="Pros-amp-Cons"><a href="#Pros-amp-Cons" class="headerlink" title="Pros &amp; Cons"></a>Pros &amp; Cons</h2><p><strong>Advantages</strong>: </p>
<ul>
<li>Good at processing <strong>Long Distant Relationship</strong> between words.</li>
<li>Can be parallel, which will result rapider training speed.</li>
</ul>
<p><strong>Disadvantages</strong>: </p>
<ul>
<li>Too much parameters.<ul>
<li>Bert: 12 layers transformer block, 1.1 million parameters.</li>
<li>Bert-large: 12 layers transformer block, 3.4 million parameters.</li>
</ul>
</li>
</ul>
<h1 id="Code"><a href="#Code" class="headerlink" title="Code"></a>Code</h1><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</span><br><span class="line"></span><br><span class="line"><span class="comment"># input_dim: 4; head_num: 2</span></span><br><span class="line">encoder_layer = nn.TransformerEncoderLayer(d_model=<span class="number">4</span>, nhead=<span class="number">2</span>)</span><br><span class="line"><span class="comment"># input size: [seq_len, batch_size, input_dim]</span></span><br><span class="line"><span class="comment"># output size: [seq_len, batch_size, input_dim]</span></span><br><span class="line">src = torch.rand(<span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>)</span><br><span class="line">out = encoder_layer(src)</span><br><span class="line"></span><br><span class="line"><span class="comment"># stacking encoder layer</span></span><br><span class="line">transformer_encoder = nn.TransformerEncoder(encoder_layer, num_layers=<span class="number">6</span>)</span><br><span class="line">output = transformer_encoder(src)</span><br><span class="line">memory = trandormer_encoder(src)</span><br><span class="line"></span><br><span class="line"><span class="comment"># decoder</span></span><br><span class="line">decoder_layer = nn.TransformerDecoderLayer(d_model=<span class="number">4</span>, nhead=<span class="number">2</span>)</span><br><span class="line">transformer_decoder = nn.TransformerDecoder(decoder_layer, num_layers=<span class="number">6</span>)</span><br><span class="line">out_part = torch.rand(<span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>)</span><br><span class="line">out = transformer_decoder(output_part, memory)</span><br></pre></td></tr></table></figure>
    </div>

    
    
    

      <footer class="post-footer">
          <div class="post-tags">
              <a href="/tags/Deep-Learning/" rel="tag"># Deep Learning</a>
          </div>

        


        
    <div class="post-nav">
      <div class="post-nav-item">
    <a href="/2022/01/16/%E5%BE%AA%E7%8E%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/" rel="prev" title="循环神经网络">
      <i class="fa fa-chevron-left"></i> 循环神经网络
    </a></div>
      <div class="post-nav-item">
    <a href="/2022/01/18/Python-API/" rel="next" title="Python API">
      Python API <i class="fa fa-chevron-right"></i>
    </a></div>
    </div>
      </footer>
    
  </article>
  
  
  



        </div>
        
    
  <div class="comments">
    <div id="lv-container" data-id="city" data-uid="MTAyMC81MDY4OC8yNzE3MQ"></div>
  </div>
  

<script>
  window.addEventListener('tabs:register', () => {
    let { activeClass } = CONFIG.comments;
    if (CONFIG.comments.storage) {
      activeClass = localStorage.getItem('comments_active') || activeClass;
    }
    if (activeClass) {
      let activeTab = document.querySelector(`a[href="#comment-${activeClass}"]`);
      if (activeTab) {
        activeTab.click();
      }
    }
  });
  if (CONFIG.comments.storage) {
    window.addEventListener('tabs:click', event => {
      if (!event.target.matches('.tabs-comment .tab-content .tab-pane')) return;
      let commentClass = event.target.classList[1];
      localStorage.setItem('comments_active', commentClass);
    });
  }
</script>

      </div>
    </main>

    <footer class="footer">
      <div class="footer-inner">
        

        

<div class="copyright">
  
  &copy; 
  <span itemprop="copyrightYear">2022</span>
  <span class="with-love">
    <i class="fa fa-heart"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">Llunch</span>
</div>
  <div class="powered-by">由 <a href="https://hexo.io/" class="theme-link" rel="noopener" target="_blank">Hexo</a> & <a href="https://theme-next.js.org/pisces/" class="theme-link" rel="noopener" target="_blank">NexT.Pisces</a> 强力驱动
  </div>

        
<div class="busuanzi-count">
  <script async src="https://busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script>
    <span class="post-meta-item" id="busuanzi_container_site_uv" style="display: none;">
      <span class="post-meta-item-icon">
        <i class="fa fa-user"></i>
      </span>
      <span class="site-uv" title="总访客量">
        <span id="busuanzi_value_site_uv"></span>
      </span>
    </span>
    <span class="post-meta-divider">|</span>
    <span class="post-meta-item" id="busuanzi_container_site_pv" style="display: none;">
      <span class="post-meta-item-icon">
        <i class="fa fa-eye"></i>
      </span>
      <span class="site-pv" title="总访问量">
        <span id="busuanzi_value_site_pv"></span>
      </span>
    </span>
</div>








      </div>
    </footer>
  </div>

  
  <script src="/lib/anime.min.js"></script>
  <script src="/lib/velocity/velocity.min.js"></script>
  <script src="/lib/velocity/velocity.ui.min.js"></script>

<script src="/js/utils.js"></script>

<script src="/js/motion.js"></script>


<script src="/js/next-boot.js"></script>


  




  
<script src="/js/local-search.js"></script>













  

  
      

<script>
  if (typeof MathJax === 'undefined') {
    window.MathJax = {
      loader: {
        source: {
          '[tex]/amsCd': '[tex]/amscd',
          '[tex]/AMScd': '[tex]/amscd'
        }
      },
      tex: {
        inlineMath: {'[+]': [['$', '$']]},
        tags: 'ams'
      },
      options: {
        renderActions: {
          findScript: [10, doc => {
            document.querySelectorAll('script[type^="math/tex"]').forEach(node => {
              const display = !!node.type.match(/; *mode=display/);
              const math = new doc.options.MathItem(node.textContent, doc.inputJax[0], display);
              const text = document.createTextNode('');
              node.parentNode.replaceChild(text, node);
              math.start = {node: text, delim: '', n: 0};
              math.end = {node: text, delim: '', n: 0};
              doc.math.push(math);
            });
          }, '', false],
          insertedScript: [200, () => {
            document.querySelectorAll('mjx-container').forEach(node => {
              let target = node.parentNode;
              if (target.nodeName.toLowerCase() === 'li') {
                target.parentNode.classList.add('has-jax');
              }
            });
          }, '', false]
        }
      }
    };
    (function () {
      var script = document.createElement('script');
      script.src = '//cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js';
      script.defer = true;
      document.head.appendChild(script);
    })();
  } else {
    MathJax.startup.document.state(0);
    MathJax.texReset();
    MathJax.typeset();
  }
</script>

    

  

<script>
NexT.utils.loadComments(document.querySelector('#lv-container'), () => {
  window.livereOptions = {
    refer: location.pathname.replace(CONFIG.root, '').replace('index.html', '')
  };
  (function(d, s) {
    var j, e = d.getElementsByTagName(s)[0];
    if (typeof LivereTower === 'function') { return; }
    j = d.createElement(s);
    j.src = 'https://cdn-city.livere.com/js/embed.dist.js';
    j.async = true;
    e.parentNode.insertBefore(j, e);
  })(document, 'script');
});
</script>

<script src="/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"pluginModelPath":"assets/","model":{"jsonPath":"/live2dw/assets/koharu.model.json"},"display":{"position":"right","width":200,"height":400},"mobile":{"show":false},"rect":"opacity:0.7","log":false,"pluginJsPath":"lib/","pluginRootPath":"live2dw/","tagMode":false});</script></body>
</html>
